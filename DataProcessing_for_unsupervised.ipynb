{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa4b3af",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbba13",
   "metadata": {},
   "source": [
    "## 0. the Introduction of the Dataset Structure\n",
    "The dataset 'stanford Sentiment Treebank' provided by tutors contains 8 files. <br>\n",
    "The following few files are important:\n",
    "1. **dataetSentence.txt**: \n",
    "2. **datasetsplit.txt**:\n",
    "3. **SOStr.txt**:\n",
    "4. **STree.txt**:\n",
    "5. **dictionary.txt**:\n",
    "6. **sentiment_labels.txt**:\n",
    "\n",
    "讲一下我们发现的每一个文件的结构，如何根据树去构造\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb6fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f796d",
   "metadata": {},
   "source": [
    "## 1.  Link content and and sentiment value by phrase_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a219d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author  : Wei Li\n",
    "# @function: Union 2 files' data by phrase id\n",
    "def union():\n",
    "    \"\"\"Return linked phrase content and data\n",
    "    save the .csv file linking label and phrase\"\"\"\n",
    "    \n",
    "    # Get phrase content and phrase id from dictionary.txt\n",
    "    with open('stanfordSentimentTreebank/dictionary.txt') as f:\n",
    "        dic_raw = f.readlines()\n",
    "    # split dataset by '|' and delete '|'\n",
    "    p_id_d = []\n",
    "    phrase_d = []\n",
    "    for i in dic_raw:\n",
    "        i = i.strip('\\n')\n",
    "        p_id_d.append((i.split('|',1))[1])\n",
    "        phrase_d.append((i.split('|',1))[0])\n",
    "    \n",
    "    # Get sentiment value and phrase id from sentiment_labels.txt\n",
    "    with open('stanfordSentimentTreebank/sentiment_labels.txt') as f:\n",
    "        sentiment_raw = f.readlines()[1:]\n",
    "    # split dataset by '|' and delete '|'\n",
    "    p_id_s = []\n",
    "    label_s = []\n",
    "    for i in sentiment_raw:\n",
    "        i = i.strip('\\n') # delete the \\n\n",
    "        p_id_s.append((i.split('|',1))[0])\n",
    "        label_s.append((i.split('|',1))[1])\n",
    "    \n",
    "    # using phrase id, link sentiment value and phrase\n",
    "    label = []\n",
    "    phrase = []\n",
    "    for i in range(len(p_id_d)):\n",
    "        index = p_id_d.index(p_id_s[i])\n",
    "        label.append(label_s[i])\n",
    "        phrase.append(phrase_d[index])\n",
    "        \n",
    "    # make columns be a DataFrame table\n",
    "    uni = pd.DataFrame({'sentiment values':label,'phrase':phrase})\n",
    "    uni.to_csv('ProccessedData/phrase_label.csv',index = False)\n",
    "    \n",
    "    # return corresponding phrase content and sentiment value\n",
    "    return(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0085706b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment values</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.44444</td>\n",
       "      <td>' (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>' ( the cockettes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.42708</td>\n",
       "      <td>' ( the cockettes )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239227</th>\n",
       "      <td>0.36111</td>\n",
       "      <td>your standard Hollywood bio-pic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239228</th>\n",
       "      <td>0.38889</td>\n",
       "      <td>your typical ` fish out of water ' story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239229</th>\n",
       "      <td>0.33333</td>\n",
       "      <td>zero .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239230</th>\n",
       "      <td>0.88889</td>\n",
       "      <td>zippy jazzy score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239231</th>\n",
       "      <td>0.5</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment values                                    phrase\n",
       "0                   0.5                                         !\n",
       "1                   0.5                                         '\n",
       "2               0.44444                                       ' (\n",
       "3                   0.5                         ' ( the cockettes\n",
       "4               0.42708                       ' ( the cockettes )\n",
       "...                 ...                                       ...\n",
       "239227          0.36111           your standard Hollywood bio-pic\n",
       "239228          0.38889  your typical ` fish out of water ' story\n",
       "239229          0.33333                                    zero .\n",
       "239230          0.88889                         zippy jazzy score\n",
       "239231              0.5                                       UNK\n",
       "\n",
       "[239232 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_label = union()\n",
    "phrase_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbb91f",
   "metadata": {},
   "source": [
    "## 2. Initialize the Structure of Trees\n",
    "**Ger data by trees' struucture**<br>\n",
    "In this dataset, every rows of the file 'SOStr' represent a sentence tree. <br>\n",
    "Every single word represents a leaf.<br>\n",
    "Every parents node in a tree represented few leaf in the tree.<br>\n",
    "The phrases in dictionary contain all node in every trees (regardless of leaves or parents nodes).<br>\n",
    "We need to get the content of nodes(every nodes in every trees) and the corresponding sentimental values.<br>\n",
    "Besides, the whole dataset's sentences are splited into 3 sub-section to train, test and validation,and a tree is represent a sentence.<br>\n",
    "- Therefore, steps are:\n",
    "    - choose a dataet\n",
    "    - get sentences in certain datasets\n",
    "    - get the nodes of (selected by levels)\n",
    "    - get the content of nodes\n",
    "    - get the corresponding sentimental value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77c91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# @author  : Wei Li\n",
    "# @function: based on a trees' sturcture, get data we need\n",
    "\n",
    "def Structure(STree,SOStr):\n",
    "    \"\"\"\n",
    "    Return total_chain, height, leaves_num, code, words.\n",
    "    total_chain: an array of paths from every leaves to root of 1 tree(leaf -> parents -> root)\n",
    "    height: the height of the tree\n",
    "    leaves_num: the number of leaves in this tree\n",
    "    code: the recording in STree which translate to list\n",
    "    words: the recording in SOStr which translate to list\n",
    "    \"\"\"   \n",
    "    # the input SOStr and STree is a single corresponding recording in SOStr.txt and STree.txt\n",
    "    # put single leaf(word) into list.\n",
    "    SOStr = SOStr.strip('\\n')\n",
    "    words = SOStr.split('|')\n",
    "    # put code value in to list.\n",
    "    STree = STree.strip('\\n')\n",
    "    code = STree.split('|')\n",
    "    # translate code value's type from str to int\n",
    "    code = [int(i) for i in code]\n",
    "    # find every paths from leaves to root, after attain root, the next value equal to 0.\n",
    "    leaves_num = len(words)\n",
    "    total_chain  = np.zeros((leaves_num,int(max(code))))\n",
    "    for i in range(leaves_num): # start from leaf...\n",
    "        chain = [i+1] # the first value of the chain is the code of this leaf in the tree\n",
    "        index = i\n",
    "        while index >= 0: \n",
    "            chain.append(code[index]) # 'code[i]' represent the parents node's code of node 'i'\n",
    "            # in python, index start from 0, but in the code of a tree, index start from 1\n",
    "            index = code[index]-1 # find the next upper parents, until it attain root\n",
    "        for j in range(len(chain)):\n",
    "            total_chain[i][j] = chain[j] # put the path into the array\n",
    "    \n",
    "    # delete unnecessary 0 value\n",
    "    idx = np.argwhere(np.all(total_chain[..., :] == 0, axis=0))\n",
    "    total_chain = np.delete(total_chain, idx, axis=1)\n",
    "    # calculate the every chains' length\n",
    "    chain_len = []\n",
    "    for i in range(leaves_num):\n",
    "        chain_len.append(len(np.nonzero(total_chain[i])[0]))\n",
    "    # calculate the height of the tree\n",
    "    height = max(chain_len)\n",
    "    return total_chain, height, leaves_num, code, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97ddb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# @author  : Wei Li\n",
    "# @function: get every nodes' words(phrases) in a tree\n",
    "\n",
    "def NodeWords(total_chain, code, words):\n",
    "    \"\"\"\n",
    "    Return the list node_words. \n",
    "    node_words is a list, every elements in this list represnt a node in a tree.\n",
    "    Every elements in this list contain all leaves belong to this node.\n",
    "    \"\"\"\n",
    "    length = len(code)\n",
    "    node_words = []\n",
    "    for i in range(length):\n",
    "        index = np.argwhere(total_chain == i+1)\n",
    "        t = ''\n",
    "        for j in range(len(index)):\n",
    "            t += words[index[j][0]] + ' '\n",
    "        t = t[:-1]\n",
    "        node_words.append(t)\n",
    "    return node_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb1c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# @author  : Wei Li\n",
    "# @function: get certain level's nodes' content(phrase) of a tree\n",
    "\n",
    "def ElementData(total_chain, height,leaves_num,code,node_words, n): \n",
    "    # n = -1: the whole tree(a sentence); n = 0: leaves; n = 1: nodes that are 1 level higher than leaves\n",
    "    if n < height and n>= 0:\n",
    "        Data = []\n",
    "        index1 = []\n",
    "        index2 = []\n",
    "        for i in range(leaves_num):\n",
    "            index1.append(int(total_chain[i][n]-1))\n",
    "        for i in index1:\n",
    "            if i not in index2 and i >= 0:\n",
    "                index2.append(int(i))\n",
    "        for i in index2:\n",
    "            Data.append(node_words[i])\n",
    "        return Data\n",
    "    if n == -1:\n",
    "        Data = node_words[max(code)-1]\n",
    "        return Data\n",
    "    else:\n",
    "        print(\"The level is out of this tree's height\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99784ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# @author  : Wei Li\n",
    "# @function: get certain level's nodes' content(phrase) of all trees\n",
    "\n",
    "def TotalData(n):\n",
    "    with open('stanfordSentimentTreebank/STree.txt') as f:\n",
    "        STree_raw = f.readlines()\n",
    "    with open('stanfordSentimentTreebank/SOStr.txt') as f:\n",
    "        SOStr_raw = f.readlines()\n",
    "        TotalData = []\n",
    "    lenth = len(SOStr_raw)\n",
    "    for i in range(lenth):\n",
    "        total_chain, height, leaves_num, code, words= Structure(STree_raw[i],SOStr_raw[i])\n",
    "        node_words = NodeWords(total_chain, code, words)\n",
    "        Data = ElementData(total_chain, height, leaves_num,code,node_words,n)\n",
    "        TotalData.append(Data)\n",
    "    return TotalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6dc5e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------- HERE ----------------------- #\n",
    "# def MakeTree(STree,SOStr):\n",
    "#     total_chain, height, leaves_num, code, words = Structure(STree,SOStr)\n",
    "#     class TreeNode:\n",
    "#         def __init__(self,x):\n",
    "#             self.value = x\n",
    "#             self.left = None\n",
    "#             self.right = None\n",
    "#     total_chain, height, leaves_num, code, words = Structure(STree,SOStr)\n",
    "#     Totol_Num = int(max(total_chain[0]))\n",
    "#     No = []\n",
    "#     for i in range(1,Totol_Num+1):\n",
    "#         No.append(str('No'+str(i)))\n",
    "#     print(No)\n",
    "#     for i in range(1,Totol_Num+1):\n",
    "#         eval(str('No'+i)) = TreeNode(i)\n",
    "#     for i in range(1,Totol_Num):\n",
    "#         TreeNode(i).left = eval(No[i-1])\n",
    "# #     for i in range(leaves_num): # rwo\n",
    "# # #         print('第',i,'行')\n",
    "# #         for j in range(1,height): # column\n",
    "# # #             print('第',j,'个')\n",
    "# #             print(total_chain[i][j])\n",
    "# #             if total_chain[i][j] != 0:\n",
    "# #                 if TreeNode(int(total_chain[i][j])).left == None:\n",
    "# #                     TreeNode(int(total_chain[i][j])).left = TreeNode(int(total_chain[i][j-1]))\n",
    "# #                 else:\n",
    "# #                     TreeNode(int(total_chain[i][j])-1).right = TreeNode(int(total_chain[i][j-1]))\n",
    "#     for i in range(1,13):\n",
    "#         print(i,':',TreeNode(i).left,TreeNode(i).right,TreeNode(i).value)\n",
    "        \n",
    "\n",
    "# SOStr = SOStr.strip('\\n')\n",
    "#     words = SOStr.split('|')\n",
    "#     # put code value in to list.\n",
    "#     STree = STree.strip('\\n')\n",
    "#     code = STree.split('|')\n",
    "\n",
    "# def LRD(STree,SOStr):\n",
    "#     total_chain, height, leaves_num, code, words = Structure(STree,SOStr)\n",
    "#     MakeTree(total_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "480a695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('stanfordSentimentTreebank/STree.txt') as f:\n",
    "#     STree_raw = f.readlines()\n",
    "# with open('stanfordSentimentTreebank/SOStr.txt') as f:\n",
    "#     SOStr_raw = f.readlines()\n",
    "# total_chain, height, leaves_num, code, words = Structure(STree_raw[133],SOStr_raw[133])  \n",
    "\n",
    "# MakeTree(STree_raw[133],SOStr_raw[133])  \n",
    "# print(total_chain)\n",
    "# # print(len(total_chain))\n",
    "# # print(height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38907b",
   "metadata": {},
   "source": [
    "## 3.  Devide the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e24c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# @author  : Wei Li\n",
    "# @function: divide dataset by datasetSplit.txt\n",
    "\n",
    "def DivideDataset(n): # n = 1 or n = 2 or n = 3\n",
    "    with open('stanfordSentimentTreebank/datasetSplit.txt') as f:\n",
    "        raw = f.readlines()[1:]\n",
    "    set = []\n",
    "    sentence_id = []\n",
    "    for i in raw:\n",
    "        i = i.strip('\\n') # delete the \\n\n",
    "        sentence_id.append((i.split(',',1))[0])\n",
    "        set.append((i.split(',',1))[1])\n",
    "    set = np.array(set,dtype='int')\n",
    "    sentence_id = np.array(sentence_id,dtype='int')\n",
    "    select_id = []\n",
    "    for i in range(len(set)):\n",
    "        if set[i] == n:\n",
    "            select_id.append(sentence_id[i])\n",
    "    return(select_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74322573",
   "metadata": {},
   "source": [
    "## 4. Choosing needed data by dataset and phrase' level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84cf7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# @author  : Wei Li\n",
    "# @function: connect data we need with corresponding label\n",
    "\n",
    "def GetDatasetContent(dataset,level):\n",
    "    Content_raw = TotalData(level)\n",
    "    Sentence_id = DivideDataset(dataset)\n",
    "    Content = []\n",
    "    for i in Sentence_id:\n",
    "        Content.append(Content_raw[i-1])\n",
    "    return(Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08a650b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindLabel(Content):\n",
    "#     with open('Unsupervised/ProccessedData/phrase_label.csv'):\n",
    "#         lp = pd.read_csv('phrase_label.csv') # label_phrase\n",
    "    lp_l = [i for i in phrase_label['sentiment values']]\n",
    "    lp_p = [i for i in phrase_label['phrase']] \n",
    "    \n",
    "    length = len(Content)\n",
    "    phrase = []\n",
    "    label = []\n",
    "    for i in range(length):\n",
    "        element = Content[i]\n",
    "        e_length = len(element)\n",
    "        for j in range(e_length):\n",
    "            index = lp_p.index(Content[i][j])\n",
    "            phrase.append(lp_p[index])\n",
    "            label.append(lp_l[index])\n",
    "    return phrase,label\n",
    "\n",
    "def FindLabelSentence(Content):\n",
    "#     with open('ProccessedData/phrase_label.csv'):\n",
    "#         lp = pd.read_csv('phrase_label.csv') # label_phrase\n",
    "    lp_l = []\n",
    "    lp_p = []\n",
    "    lp_l = [i for i in phrase_label['sentiment values']]\n",
    "    lp_p = [i for i in phrase_label['phrase']] \n",
    "    length = len(Content)\n",
    "    phrase = []\n",
    "    label = []\n",
    "    for i in range(length):\n",
    "        element = Content[i]\n",
    "        index = lp_p.index(element)\n",
    "        phrase.append(lp_p[index])\n",
    "        label.append(lp_l[index])\n",
    "    return phrase,label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913def0",
   "metadata": {},
   "source": [
    "## 5. Get Specific Data, Save Them into Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a46e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 1\n",
    "Singal_word_1 = GetDatasetContent(1,0)\n",
    "Level1_1 = GetDatasetContent(1,1)\n",
    "Sentences_1 = GetDatasetContent(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a080172",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_s1,label_s1 = FindLabelSentence(Sentences_1)\n",
    "df= pd.DataFrame({'label':label_s1,'phrase':phrase_s1})\n",
    "df.to_csv(\"ProccessedData/sentence_dataset1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f36e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_01,label_01 = FindLabel(Singal_word_1)\n",
    "df= pd.DataFrame({'label':label_01,'phrase':phrase_01})\n",
    "df.to_csv(\"ProccessedData/level0_dataset1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c39d47",
   "metadata": {},
   "source": [
    "## 6. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a46b0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(Doc_Name:'str') :#-> list: \n",
    "    # get the file path\n",
    "    stopwords_dict = os.listdir(\"stopwords/\")\n",
    "    # add stop words from files to list\n",
    "    stop_list = []\n",
    "    for dic in stopwords_dict:\n",
    "        stop_list.append([line.strip() for line in open('stopwords/' + dic, 'r').readlines()])\n",
    "    \n",
    "    # delete second bracket\n",
    "    stop_words = ' '.join(str(i) for i in stop_list)\n",
    "\n",
    "    # get raw data from dataset(single word)\n",
    "    with open (Doc_Name):\n",
    "        raw_data = pd.read_csv(Doc_Name,encoding='gbk')\n",
    "    \n",
    "    # stop word clean\n",
    "    clean_phrase = []\n",
    "    clean_label = []\n",
    "\n",
    "    for i in range(len(raw_data)):\n",
    "        if raw_data['phrase'][i] not in stop_words:\n",
    "            clean_phrase.append(raw_data['phrase'][i])\n",
    "            clean_label.append(raw_data['label'][i])\n",
    "    \n",
    "    # save cleaned file\n",
    "    df= pd.DataFrame({'label':clean_label,'phrase':clean_phrase})\n",
    "    df.to_csv(Doc_Name[:-4]+'_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f2f1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_repetition(Doc_Name:'str'):\n",
    "    with open (Doc_Name):\n",
    "        raw_data = pd.read_csv(Doc_Name) #encoding='gbk'\n",
    "    clean_phrase = []\n",
    "    clean_label = []\n",
    "    test2 = []\n",
    "    for i in range(len(raw_data)):\n",
    "        if raw_data['phrase'][i] not in clean_phrase:\n",
    "            clean_phrase.append(raw_data['phrase'][i])\n",
    "            clean_label.append(raw_data['label'][i])\n",
    "        else:\n",
    "            test2.append(raw_data['phrase'][i])\n",
    "    df= pd.DataFrame({'label':clean_label,'phrase':clean_phrase})\n",
    "    df.to_csv(Doc_Name[:-4]+'_cle_rep.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33faeffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
